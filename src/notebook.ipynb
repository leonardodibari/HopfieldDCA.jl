{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DCAUtils, HopfieldDCA, Tullio, BenchmarkTools, Zygote, LinearAlgebra\n",
    "Z = read_fasta_alignment(\"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\",0); W, M_eff = DCAUtils.compute_weights(Z,0.2);@tullio delta_j[a, j, m] := a .== Z[j, m] (a in 1:20); @tullio delta_i[a, i, m] := a .== Z[i, m] (a in 1:20);N = size(Z)[1]; M = size(Z)[2]; q = 20;  H = 10; lambdaK=5;lambdaV=5;delta_la = Matrix(I,q,q) ; alg_var = HopPlmVar(N,M_eff,q,H,lambdaK, lambdaV, Z,W, delta_i, delta_j, delta_la);K = rand(N,N,H); V = rand(q, H); g_K = rand(N*N*H); g_V = rand(q*H);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1,b1 = gradient((p1,p2)->get_loss_and_grad_zyg(p1, p2, alg_var),K,V); a,b,l = get_loss_and_grad(K, V, alg_var); println(sum(a .- a1)); println(sum(b.-b1));\n",
    "#works well on gradient on k, not on be, maybe add regularization and the stretching of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HopfieldDCA, BenchmarkTools, DCAUtils, PyPlot, Zygote\n",
    "filepath = \"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\"; M = 1000;\n",
    "lambdaK = 0.5; lambdaV = lambdaK;  H = 2; alg_var = define_var(filepath,H,0.5,0.5);#alg_var = define_var(filepath,H,0.5,0.5,M);\n",
    "@time grad = check_with_zyg(alg_var);\n",
    "close(\"all\"); scatter(grad[1], grad[2]); gcf();\n",
    "\n",
    "l_r = 0.005; n_epoch = 100; print_rate = 5;trainer_small(n_epoch, l_r, l_r, alg_var, print_rate, false);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HopfieldDCA#, BenchmarkTools#, DCAUtils, PyPlot, Zygote, Tullio\n",
    "using DCAUtils\n",
    "filepath = \"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\"; Z = read_fasta_alignment(filepath,0.99);#[:,1:100];\n",
    "lambdaK = 0.005; lambdaV = lambdaK;  H = 2; alg_var = HopPlmVar(H,lambdaK,lambdaV, Z);\n",
    "#lambdaK = 0.005; lambdaV = lambdaK;  H = 2; alg_var = define_var(filepath,H,lambdaK,lambdaV);\n",
    "K = rand(alg_var.N, alg_var.N, alg_var.H); V = rand(alg_var.q, alg_var.H); tmp = StgArr(alg_var);\n",
    "en = rand(alg_var.q, alg_var.N, length(alg_var.W)); data_en = rand(alg_var.N, length(alg_var.W));\n",
    "loss = rand(alg_var.N) ; reg_k = rand(alg_var.N);\n",
    "@benchmark lz = get_loss_and_grad_zyg(K, V, alg_var)\n",
    "@benchmark lz2 = get_loss_and_grad_zyg2(K, V, alg_var, tmp)\n",
    "@benchmark lz3 = get_loss_and_grad_zyg3($K, $V, $alg_var, $en, $data_en, $loss, $reg_k)\n",
    "@time az,bz = gradient((p1,p2)->get_loss_and_grad_zyg(p1, p2, alg_var),K,V);\n",
    "@time az2,bz2 = gradient((p1,p2)->get_loss_and_grad_zyg2(p1, p2, alg_var, tmp),K,V);\n",
    "println([sum(az .-az2),sum(bz .-bz2),lz-lz2])\n",
    "\n",
    "K = rand(alg_var.N, alg_var.N, alg_var.H); V = rand(alg_var.q, alg_var.H); tmp = StgArr(alg_var);\n",
    "en = rand(alg_var.q, alg_var.N, length(alg_var.W)); data_en = rand(alg_var.N, length(alg_var.W));\n",
    "loss = rand(alg_var.N) ; reg_k = rand(alg_var.N);\n",
    "@time a,b,l = get_loss_and_grad(K, V, alg_var);\n",
    "@time a2,b2,l2 = get_loss_and_grad2(K, V, alg_var, tmp);\n",
    "@time az,bz = gradient((p1,p2)->get_loss_and_grad_zyg(p1, p2, alg_var),K,V);\n",
    "println([sum(a .-a2), sum(b .-b2), l-l2])\n",
    "println([sum(a .-az), sum(b .-bz)])\n",
    "\n",
    "using HopfieldDCA; import DCAUtils: read_fasta_alignment;#, BenchmarkTools#, DCAUtils, PyPlot, Zygote, Tullio\n",
    "filepath = \"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\"; Z = read_fasta_alignment(filepath,0.99);#[:,1:100];\n",
    "lambdaK = 0.005; lambdaV = lambdaK;  H = 2; alg_var = HopPlmVar(H,lambdaK,lambdaV, Z);\n",
    "#lambdaK = 0.005; lambdaV = lambdaK;  H = 2; alg_var = define_var(filepath,H,lambdaK,lambdaV);\n",
    "K = rand(alg_var.N, alg_var.N, alg_var.H); V = rand(alg_var.q, alg_var.H); tmp = StgArr(alg_var);\n",
    "@time a,b,l = get_loss_and_grad(K, V, alg_var);\n",
    "\n",
    "\n",
    "@benchmark lz = [get_loss_and_grad_zyg(K, V, alg_var) for i in 1:10]\n",
    "@benchmark lz2 = [get_loss_and_grad_zyg2(K, V, alg_var, tmp) for i in 1:10]\n",
    "@benchmark lz3 = [get_loss_and_grad_zyg3(K, V, alg_var, en, data_en, loss, reg_k) for i in 1:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HopfieldDCA\n",
    "filepath = \"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\"; lambdaK = 0.005; lambdaV = lambdaK;  \n",
    "H = 1; alg_var = define_var(filepath,H,lambdaK,lambdaV); m = trainer(alg_var, 5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using HopfieldDCA\n",
    "filepath = \"../DataAttentionDCA\\\\data\\\\PF00014\\\\PF00014_mgap6.fasta.gz\"; \n",
    "\n",
    "for h in [2, 10, 30, 50, 70]\n",
    "    for lambda in [10^-5, 10^-3, 10^-2, 10^-1]\n",
    "\n",
    "        lambdaK = lambda; lambdaV = lambdaK; alg_var = define_var(filepath,h,lambdaK,lambdaV);\n",
    "        @time m = trainer(alg_var, 100);\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
